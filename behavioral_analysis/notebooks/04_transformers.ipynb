{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8c41e1",
   "metadata": {},
   "source": [
    "# Sprint 4: Transformer Fine-tuning (RoBERTa)\n",
    "\n",
    "**Objective**: Fine-tune RoBERTa on FakeNewsNet titles to leverage deep semantic understanding.\n",
    "\n",
    "**Hypothesis**: Pre-trained transformers, which understand contextual meaning and nuance, should outperform classical methods (TF-IDF + behavioral features) that rely on bag-of-words representations.\n",
    "\n",
    "**Baseline to Beat**: 0.8621 ROC-AUC (Hybrid model: TF-IDF + linguistic features)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "REPO_DIR = Path('../../')\n",
    "DATA_PATH = REPO_DIR / 'data' / 'processed'\n",
    "MODEL_DIR = REPO_DIR / 'models'\n",
    "RESULTS_DIR = REPO_DIR / 'results'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521f215",
   "metadata": {},
   "source": [
    "## 1. Load Fine-tuned Model & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles and features\n",
    "df = pd.read_parquet(DATA_PATH / 'articles.parquet')\n",
    "features = pd.read_parquet(DATA_PATH / 'features.parquet')\n",
    "\n",
    "df['label_num'] = (df['label'] == 'fake').astype(int)\n",
    "\n",
    "# Create same 80/20 split as training\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, val_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label_num'], random_state=42\n",
    ")\n",
    "\n",
    "print(f'Validation set: {len(val_df)} articles')\n",
    "print(f'Class distribution:\\n{val_df[\"label_num\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe808657",
   "metadata": {},
   "source": [
    "## 2. Load Fine-tuned RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR / 'roberta_tokenizer'))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    str(MODEL_DIR / 'roberta_fine_tuned'),\n",
    "    num_labels=2,\n",
    "    device_map='auto' if DEVICE == 'cuda' else None\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print('✅ Model loaded successfully')\n",
    "print(f'Model size: {model.num_parameters():,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2c2ca",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch predict\n",
    "pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if DEVICE == 'cuda' else -1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "results = pipe(val_df['title'].tolist())\n",
    "\n",
    "# Extract probabilities\n",
    "y_pred = np.array([1 if r['label'] == 'LABEL_1' else 0 for r in results])\n",
    "y_pred_proba = np.array([r['score'] for r in results])\n",
    "\n",
    "# For LABEL_0 predictions, flip probability\n",
    "y_pred_proba = np.where(\n",
    "    [r['label'] == 'LABEL_1' for r in results],\n",
    "    y_pred_proba,\n",
    "    1 - y_pred_proba\n",
    ")\n",
    "\n",
    "y_true = val_df['label_num'].values\n",
    "\n",
    "print(f'Generated {len(y_pred)} predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9f65b",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8247b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('ROBERTA TRANSFORMER PERFORMANCE')\n",
    "print('='*70)\n",
    "print(f'Accuracy:  {accuracy:.4f}')\n",
    "print(f'F1 Score:  {f1:.4f}')\n",
    "print(f'ROC-AUC:   {roc_auc:.4f}')\n",
    "print(f'\\nConfusion Matrix:')\n",
    "print(cm)\n",
    "print(f'\\nTrue Negatives: {cm[0,0]} | False Positives: {cm[0,1]}')\n",
    "print(f'False Negatives: {cm[1,0]} | True Positives: {cm[1,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607829c6",
   "metadata": {},
   "source": [
    "## 5. Comparison with Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison data\n",
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'Behavioral\\nOnly',\n",
    "        'TF-IDF\\nBaseline',\n",
    "        'Hybrid\\n(TF-IDF+Behav)',\n",
    "        'RoBERTa\\nTransformer'\n",
    "    ],\n",
    "    'Accuracy': [0.5436, 0.8120, 0.8097, accuracy],\n",
    "    'F1 Score': [0.3930, 0.6443, 0.6412, f1],\n",
    "    'ROC-AUC': [0.6054, 0.8590, 0.8621, roc_auc]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('\\n' + '='*70)\n",
    "print('MODEL COMPARISON ACROSS ALL SPRINTS')\n",
    "print('='*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_auc = 0.8590\n",
    "improvement_pct = ((roc_auc - baseline_auc) / baseline_auc) * 100\n",
    "print(f'\\nImprovement over TF-IDF baseline: {improvement_pct:+.2f}%')\n",
    "print(f'Absolute improvement: {(roc_auc - baseline_auc):+.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69d493",
   "metadata": {},
   "source": [
    "## 6. ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# RoBERTa ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "ax.plot(fpr, tpr, color='#d62728', lw=3, label=f'RoBERTa (AUC={roc_auc:.4f})')\n",
    "\n",
    "# Baseline ROC curves for reference\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('RoBERTa Transformer ROC Curve', fontweight='bold', fontsize=13)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'roberta_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Saved ROC curve visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1ff20",
   "metadata": {},
   "source": [
    "## 7. Error Analysis - Where Does RoBERTa Fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify errors\n",
    "val_df_with_pred = val_df.copy()\n",
    "val_df_with_pred['y_pred'] = y_pred\n",
    "val_df_with_pred['y_pred_proba'] = y_pred_proba\n",
    "val_df_with_pred['correct'] = (val_df_with_pred['label_num'] == y_pred)\n",
    "\n",
    "false_positives = val_df_with_pred[\n",
    "    (val_df_with_pred['label_num'] == 0) & (val_df_with_pred['y_pred'] == 1)\n",
    "]\n",
    "false_negatives = val_df_with_pred[\n",
    "    (val_df_with_pred['label_num'] == 1) & (val_df_with_pred['y_pred'] == 0)\n",
    "]\n",
    "\n",
    "print(f'\\nFalse Positives (Real → Predicted Fake): {len(false_positives)}')\n",
    "print(f'False Negatives (Fake → Predicted Real): {len(false_negatives)}')\n",
    "print(f'\\nTop 5 High-Confidence False Positives (real but predicted fake):')\n",
    "print(false_positives.nlargest(5, 'y_pred_proba')[['title', 'label', 'y_pred_proba']].to_string())\n",
    "print(f'\\nTop 5 High-Confidence False Negatives (fake but predicted real):')\n",
    "print(false_negatives.nsmallest(5, 'y_pred_proba')[['title', 'label', 'y_pred_proba']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2a4a0",
   "metadata": {},
   "source": [
    "## 8. Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91405a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution by true label\n",
    "ax1.hist([y_pred_proba[y_true == 0], y_pred_proba[y_true == 1]],\n",
    "         bins=30, label=['Real News', 'Fake News'], alpha=0.7, color=['blue', 'red'])\n",
    "ax1.set_xlabel('Predicted Probability of Being Fake', fontsize=11)\n",
    "ax1.set_ylabel('Frequency', fontsize=11)\n",
    "ax1.set_title('RoBERTa Confidence Distribution', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Distribution by prediction correctness\n",
    "correct = val_df_with_pred['correct'].values\n",
    "ax2.hist([y_pred_proba[correct], y_pred_proba[~correct]],\n",
    "         bins=30, label=['Correct', 'Incorrect'], alpha=0.7, color=['green', 'orange'])\n",
    "ax2.set_xlabel('Predicted Probability', fontsize=11)\n",
    "ax2.set_ylabel('Frequency', fontsize=11)\n",
    "ax2.set_title('Confidence: Correct vs Incorrect Predictions', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'roberta_confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Saved confidence distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8971f",
   "metadata": {},
   "source": [
    "## 9. Key Insights & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('SPRINT 4 KEY FINDINGS')\n",
    "print('='*70)\n",
    "\n",
    "if roc_auc > 0.8621:\n",
    "    print(f'\\n✅ TRANSFORMER BREAKTHROUGH!')\n",
    "    print(f'   RoBERTa ROC-AUC: {roc_auc:.4f}')\n",
    "    print(f'   Improvement: {improvement_pct:+.2f}% over hybrid model')\n",
    "    print(f'\\n   Semantic understanding (BERT/RoBERTa) outperformed')\n",
    "    print(f'   classical bag-of-words approaches.')\n",
    "else:\n",
    "    print(f'\\n⚠️  MARGINAL IMPROVEMENT')\n",
    "    print(f'   RoBERTa ROC-AUC: {roc_auc:.4f}')\n",
    "    print(f'   Improvement: {improvement_pct:+.2f}% over hybrid model')\n",
    "    print(f'\\n   Semantic understanding provides only marginal gains.')\n",
    "    print(f'   Suggests title-based classification has inherent ceiling.')\n",
    "\n",
    "print(f'\\nFALSE POSITIVES: {len(false_positives)} real articles flagged as fake')\n",
    "print(f'FALSE NEGATIVES: {len(false_negatives)} fake articles flagged as real')\n",
    "\n",
    "print(f'\\nNEXT STEPS:')\n",
    "print(f'  1. Analyze error patterns: common topics in FP/FN')\n",
    "print(f'  2. Attention visualization: which words drive predictions?')\n",
    "print(f'  3. Domain adaptation: fine-tune on full article text (if available)')\n",
    "print(f'  4. Ensemble: combine RoBERTa + classical models')\n",
    "print(f'  5. Production pipeline: model serving + API integration')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
