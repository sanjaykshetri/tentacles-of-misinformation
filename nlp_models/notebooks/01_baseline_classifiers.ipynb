{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7961b090",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP preprocessing\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3648885",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Misinformation Dataset\n",
    "\n",
    "*(In production: Download LIAR or FakeNewsNet)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For capstone: create realistic synthetic misinformation data\n",
    "# In production: Use actual LIAR or FakeNewsNet datasets\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Misinformation statements (synthetic examples)\n",
    "false_statements = [\n",
    "    \"5G towers cause COVID-19 transmission\",\n",
    "    \"Vaccines contain microchips for tracking\",\n",
    "    \"The moon landing was faked\",\n",
    "    \"Climate change is a hoax\",\n",
    "    \"Chemtrails control weather\",\n",
    "    \"Earth is flat and NASA lies\",\n",
    "    \"Vaccines caused the autism crisis\",\n",
    "    \"JFK assassination was conspiracy\",\n",
    "]\n",
    "\n",
    "# True statements (synthetic examples)\n",
    "true_statements = [\n",
    "    \"Water freezes at 0 degrees Celsius\",\n",
    "    \"The Earth orbits the Sun\",\n",
    "    \"Vaccines have saved millions of lives\",\n",
    "    \"Carbon dioxide causes climate change\",\n",
    "    \"DNA is the molecule of heredity\",\n",
    "    \"Gravity pulls objects downward\",\n",
    "    \"The sun is a star\",\n",
    "    \"Sound travels slower than light\",\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "n_samples = 500\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i in range(n_samples // 2):\n",
    "    texts.append(np.random.choice(false_statements) + f\" statement variant {i}\")\n",
    "    labels.append('false')\n",
    "    \n",
    "    texts.append(np.random.choice(true_statements) + f\" statement variant {i}\")\n",
    "    labels.append('true')\n",
    "\n",
    "df_news = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'label': labels,\n",
    "    'id': range(1, len(texts) + 1)\n",
    "})\n",
    "\n",
    "# Shuffle\n",
    "df_news = df_news.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Created synthetic misinformation dataset: {len(df_news)} samples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_news['label'].value_counts())\n",
    "print(f\"\\nFirst 5 examples:\")\n",
    "print(df_news.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa409166",
   "metadata": {},
   "source": [
    "## 3. Data Quality & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(\"Missing data:\")\n",
    "print(df_news.isnull().sum())\n",
    "\n",
    "# Text statistics\n",
    "df_news['text_length'] = df_news['text'].str.len()\n",
    "df_news['word_count'] = df_news['text'].str.split().str.len()\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(f\"Mean length: {df_news['text_length'].mean():.0f} chars, SD = {df_news['text_length'].std():.0f}\")\n",
    "print(f\"Mean word count: {df_news['word_count'].mean():.1f}, SD = {df_news['word_count'].std():.1f}\")\n",
    "print(f\"Length range: {df_news['text_length'].min()} - {df_news['text_length'].max()} chars\")\n",
    "\n",
    "# Class balance\n",
    "print(f\"\\nClass balance:\")\n",
    "class_dist = df_news['label'].value_counts()\n",
    "print(class_dist)\n",
    "print(f\"Balance ratio: {class_dist['true'] / class_dist['false']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a298933",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86f558",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_news['text_cleaned'],\n",
    "    df_news['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_news['label']\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb7670",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fecc64",
   "metadata": {},
   "source": [
    "## 7. Baseline Model 1: Logistic Regression + TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6df23",
   "metadata": {},
   "source": [
    "# Train Naive Bayes\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb = clf_nb.predict(X_test_tfidf)\n",
    "y_pred_proba_nb = clf_nb.predict_proba(X_test_tfidf)[:, 1]  # Probability of 'true'\n",
    "\n",
    "# Evaluation\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "prec_nb = precision_score(y_test, y_pred_nb, pos_label='true')\n",
    "rec_nb = recall_score(y_test, y_pred_nb, pos_label='true')\n",
    "f1_nb = f1_score(y_test, y_pred_nb, pos_label='true')\n",
    "auc_nb = roc_auc_score(y_test.map({'true': 1, 'false': 0}), y_pred_proba_nb)\n",
    "\n",
    "print(\"Naive Bayes + TF-IDF Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {acc_nb:.4f}\")\n",
    "print(f\"Precision: {prec_nb:.4f}\")\n",
    "print(f\"Recall:    {rec_nb:.4f}\")\n",
    "print(f\"F1-Score:  {f1_nb:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc_nb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b7d88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d95aa45",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12804e83",
   "metadata": {},
   "source": [
    "# ROC curves\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "y_test_binary = y_test.map({'true': 1, 'false': 0})\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test_binary, y_pred_proba_lr)\n",
    "ax.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC={auc_lr:.3f})', linewidth=2)\n",
    "\n",
    "# Naive Bayes\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test_binary, y_pred_proba_nb)\n",
    "ax.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC={auc_nb:.3f})', linewidth=2)\n",
    "\n",
    "# SVM\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test_binary, y_decision_svm)\n",
    "ax.plot(fpr_svm, tpr_svm, label=f'Linear SVM (AUC={auc_svm:.3f})', linewidth=2)\n",
    "\n",
    "# Diagonal (random classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax.set_title('ROC Curves: Baseline Models', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/07_baseline_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: docs/figures/07_baseline_roc_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c8f17",
   "metadata": {},
   "source": [
    "# Get feature importance from Logistic Regression coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': clf_lr.coef_[0]\n",
    "})\n",
    "\n",
    "# Top 15 features (by absolute coefficient)\n",
    "top_features = feature_importance.reindex(\n",
    "    feature_importance['coefficient'].abs().argsort()[-15:]\n",
    ").sort_values('coefficient', ascending=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['red' if x < 0 else 'green' for x in top_features['coefficient']]\n",
    "ax.barh(top_features['feature'], top_features['coefficient'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Coefficient (importance)', fontsize=11)\n",
    "ax.set_title('Top 15 Features: Logistic Regression\\n(Red=predict false, Green=predict true)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/08_baseline_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: docs/figures/08_baseline_feature_importance.png\")\n",
    "print(f\"\\nTop 10 features predicting 'true' (misinformation):\")\n",
    "print(top_features[top_features['coefficient'] > 0].tail(10)[['feature', 'coefficient']].to_string(index=False))\n",
    "print(f\"\\nTop 10 features predicting 'false' (credible):\")\n",
    "print(top_features[top_features['coefficient'] < 0].head(10)[['feature', 'coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae29a46",
   "metadata": {},
   "source": [
    "# Create error analysis dataframe\n",
    "errors_df = pd.DataFrame({\n",
    "    'text': X_test.values,\n",
    "    'true_label': y_test.values,\n",
    "    'pred_label': y_pred_lr,\n",
    "    'confidence': np.max(clf_lr.predict_proba(X_test_tfidf), axis=1)\n",
    "})\n",
    "\n",
    "errors_df['is_correct'] = errors_df['true_label'] == errors_df['pred_label']\n",
    "errors_df['is_false_positive'] = (errors_df['true_label'] == 'false') & (errors_df['pred_label'] == 'true')\n",
    "errors_df['is_false_negative'] = (errors_df['true_label'] == 'true') & (errors_df['pred_label'] == 'false')\n",
    "\n",
    "# Error statistics\n",
    "print(\"Error Analysis (Logistic Regression):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Correct predictions: {errors_df['is_correct'].sum()} / {len(errors_df)}\")\n",
    "print(f\"False Positives (predicted true, actually false): {errors_df['is_false_positive'].sum()}\")\n",
    "print(f\"False Negatives (predicted false, actually true): {errors_df['is_false_negative'].sum()}\")\n",
    "\n",
    "print(f\"\\nExamples of False Positives (Model too confident misinformation):\")\n",
    "fp_examples = errors_df[errors_df['is_false_positive']].nlargest(3, 'confidence')\n",
    "for idx, row in fp_examples.iterrows():\n",
    "    print(f\"  Text: {row['text'][:60]}... | Confidence: {row['confidence']:.3f}\")\n",
    "\n",
    "print(f\"\\nExamples of False Negatives (Model missed misinformation):\")\n",
    "fn_examples = errors_df[errors_df['is_false_negative']].nsmallest(3, 'confidence')\n",
    "for idx, row in fn_examples.iterrows():\n",
    "    print(f\"  Text: {row['text'][:60]}... | Confidence: {row['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20954bfa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
