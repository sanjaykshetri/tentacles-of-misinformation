{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a8fe2e",
   "metadata": {},
   "source": [
    "# FakeNewsNet Exploratory Data Analysis\n",
    "\n",
    "**Sprint 1**: Data ingestion, cleaning, and exploration of the FakeNewsNet dataset.\n",
    "\n",
    "Dataset: Labeled news articles from Politifact and GossipCop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e42eac",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc325026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(\"../../data/processed/articles.parquet\")\n",
    "print(f\"Loaded {len(df)} articles\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f81a9c",
   "metadata": {},
   "source": [
    "## 2. Class Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0dbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df['label'].value_counts()\n",
    "class_props = df['label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)\n",
    "print(\"\\nProportions:\")\n",
    "print(class_props)\n",
    "print(f\"\\nImbalance ratio (Real:Fake): {class_counts['real'] / class_counts['fake']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "class_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Article Count by Label', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "for i, v in enumerate(class_counts):\n",
    "    axes[0].text(i, v + 200, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[1].pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Class Distribution by Dataset\")\n",
    "print(\"=\"*50)\n",
    "print(pd.crosstab(df['dataset'], df['label'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1419457",
   "metadata": {},
   "source": [
    "## 3. Title Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ac3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics by label\n",
    "print(\"Title Length Statistics by Label:\")\n",
    "print(df.groupby('label')[['title_length', 'title_chars']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histogram - Title Length\n",
    "for label in ['real', 'fake']:\n",
    "    axes[0, 0].hist(df[df['label'] == label]['title_length'], \n",
    "                    alpha=0.6, label=label, bins=30)\n",
    "axes[0, 0].set_xlabel('Words in Title')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Title Length Distribution', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Boxplot - Title Length\n",
    "sns.boxplot(data=df, x='label', y='title_length', ax=axes[0, 1], \n",
    "            palette=['#2ecc71', '#e74c3c'])\n",
    "axes[0, 1].set_title('Title Length by Label', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Label')\n",
    "axes[0, 1].set_ylabel('Words in Title')\n",
    "\n",
    "# Histogram - Title Characters\n",
    "for label in ['real', 'fake']:\n",
    "    axes[1, 0].hist(df[df['label'] == label]['title_chars'], \n",
    "                    alpha=0.6, label=label, bins=30)\n",
    "axes[1, 0].set_xlabel('Characters in Title')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Title Character Length Distribution', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Boxplot - Title Characters\n",
    "sns.boxplot(data=df, x='label', y='title_chars', ax=axes[1, 1], \n",
    "            palette=['#2ecc71', '#e74c3c'])\n",
    "axes[1, 1].set_title('Title Character Length by Label', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Label')\n",
    "axes[1, 1].set_ylabel('Characters in Title')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22aca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test for title length differences\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "real_titles = df[df['label'] == 'real']['title_length']\n",
    "fake_titles = df[df['label'] == 'fake']['title_length']\n",
    "\n",
    "t_stat, p_value = ttest_ind(real_titles, fake_titles)\n",
    "print(f\"\\nT-test: Title Length (Real vs Fake)\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.2e}\")\n",
    "print(f\"Significant? {p_value < 0.05}\")\n",
    "print(f\"\\nMean title length - Real: {real_titles.mean():.2f}, Fake: {fake_titles.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e4625",
   "metadata": {},
   "source": [
    "## 4. Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Extract top N words from text.\"\"\"\n",
    "    vectorizer = CountVectorizer(max_features=1000, stop_words='english', \n",
    "                                lowercase=True, min_df=2)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    word_freq = np.array(X.sum(axis=0)).flatten()\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    sorted_idx = np.argsort(word_freq)[::-1]\n",
    "    top_words = [(words[i], word_freq[i]) for i in sorted_idx[:n]]\n",
    "    return pd.DataFrame(top_words, columns=['word', 'frequency'])\n",
    "\n",
    "# Real articles\n",
    "real_words = get_top_words(df[df['label'] == 'real']['title'], n=20)\n",
    "print(\"\\nTop 20 Words in REAL Articles:\")\n",
    "print(real_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed804e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake articles\n",
    "fake_words = get_top_words(df[df['label'] == 'fake']['title'], n=20)\n",
    "print(\"\\nTop 20 Words in FAKE Articles:\")\n",
    "print(fake_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Real\n",
    "axes[0].barh(real_words['word'].iloc[::-1], real_words['frequency'].iloc[::-1], \n",
    "             color='#2ecc71')\n",
    "axes[0].set_title('Top 20 Words - REAL Articles', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Frequency')\n",
    "\n",
    "# Fake\n",
    "axes[1].barh(fake_words['word'].iloc[::-1], fake_words['frequency'].iloc[::-1], \n",
    "             color='#e74c3c')\n",
    "axes[1].set_title('Top 20 Words - FAKE Articles', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d45bd",
   "metadata": {},
   "source": [
    "## 5. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nâœ… Total articles: {len(df):,}\")\n",
    "print(f\"âœ… Real: {(df['label']=='real').sum():,} ({(df['label']=='real').sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"âœ… Fake: {(df['label']=='fake').sum():,} ({(df['label']=='fake').sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nâœ… Dataset sources:\")\n",
    "print(df['dataset'].value_counts())\n",
    "\n",
    "print(f\"\\nâœ… Title length range: {df['title_length'].min():.0f} - {df['title_length'].max():.0f} words\")\n",
    "print(f\"âœ… Title character range: {df['title_chars'].min():.0f} - {df['title_chars'].max():.0f} chars\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for modeling! ðŸš€\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
