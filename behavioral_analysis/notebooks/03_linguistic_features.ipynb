{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0dda4fc",
   "metadata": {},
   "source": [
    "# Sprint 3: Linguistic & Behavioral Features for Misinformation Detection\n",
    "\n",
    "**Objective**: Test whether psychologically-grounded linguistic features improve model performance beyond bag-of-words.\n",
    "\n",
    "**Experiment Design**:\n",
    "1. **Baseline**: TF-IDF only (ROC-AUC = 0.859)\n",
    "2. **Behavioral Features Only**: Sentiment, subjectivity, readability, certainty language\n",
    "3. **Hybrid Model**: TF-IDF + Behavioral Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11214e7",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, auc, precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(\"../../data/processed/articles.parquet\")\n",
    "df[\"label_num\"] = (df[\"label\"] == \"fake\").astype(int)\n",
    "feats = pd.read_parquet(\"../../data/processed/features.parquet\")\n",
    "\n",
    "# Combine\n",
    "df = pd.concat([df.reset_index(drop=True), feats.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(f\"Loaded {len(df)} articles with {feats.shape[1]} behavioral features\")\n",
    "print(f\"\\nFeature columns: {feats.columns.tolist()}\")\n",
    "print(f\"\\nData shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2723d",
   "metadata": {},
   "source": [
    "## 2. Feature Correlation with Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61330b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of behavioral features with label\n",
    "feature_cols = feats.columns.tolist()\n",
    "corr_with_label = df[feature_cols + ['label_num']].corr()['label_num'][:-1].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nCorrelation of Behavioral Features with Fake Label:\")\n",
    "print(corr_with_label)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "corr_with_label.plot(kind='barh', ax=ax, color=['green' if x > 0 else 'red' for x in corr_with_label])\n",
    "ax.set_title('Feature Correlation with Fake Label', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Correlation Coefficient')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abbd0a",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[[\"title\"] + feature_cols],\n",
    "    df[\"label_num\"],\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label_num\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train):,}\")\n",
    "print(f\"Val size: {len(X_val):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a94e23",
   "metadata": {},
   "source": [
    "## 4. Behavioral-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbdc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_behav = scaler.fit_transform(X_train[feature_cols])\n",
    "X_val_behav = scaler.transform(X_val[feature_cols])\n",
    "\n",
    "# Train\n",
    "print(\"Training Behavioral-Only Model...\")\n",
    "model_behav = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42)\n",
    "model_behav.fit(X_train_behav, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_behav = model_behav.predict(X_val_behav)\n",
    "y_pred_proba_behav = model_behav.predict_proba(X_val_behav)[:, 1]\n",
    "roc_auc_behav = roc_auc_score(y_val, y_pred_proba_behav)\n",
    "f1_behav = f1_score(y_val, y_pred_behav)\n",
    "acc_behav = accuracy_score(y_val, y_pred_behav)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BEHAVIORAL-ONLY MODEL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {acc_behav:.4f}\")\n",
    "print(f\"F1 Score: {f1_behav:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_behav:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_behav, target_names=[\"Real\", \"Fake\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8d17a",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Model (Baseline - Load Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained vectorizer and model\n",
    "vectorizer = joblib.load(\"../../models/tfidf_vectorizer.joblib\")\n",
    "model_tfidf = joblib.load(\"../../models/logistic_regression.joblib\")\n",
    "\n",
    "# Vectorize\n",
    "X_train_tfidf = vectorizer.transform(X_train[\"title\"])\n",
    "X_val_tfidf = vectorizer.transform(X_val[\"title\"])\n",
    "\n",
    "# Evaluate\n",
    "y_pred_tfidf = model_tfidf.predict(X_val_tfidf)\n",
    "y_pred_proba_tfidf = model_tfidf.predict_proba(X_val_tfidf)[:, 1]\n",
    "roc_auc_tfidf = roc_auc_score(y_val, y_pred_proba_tfidf)\n",
    "f1_tfidf = f1_score(y_val, y_pred_tfidf)\n",
    "acc_tfidf = accuracy_score(y_val, y_pred_tfidf)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"TF-IDF BASELINE (SPRINT 2)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {acc_tfidf:.4f}\")\n",
    "print(f\"F1 Score: {f1_tfidf:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d1768",
   "metadata": {},
   "source": [
    "## 6. Hybrid Model: TF-IDF + Behavioral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d684fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine TF-IDF + behavioral features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train_hybrid = hstack([X_train_tfidf, X_train_behav])\n",
    "X_val_hybrid = hstack([X_val_tfidf, X_val_behav])\n",
    "\n",
    "print(f\"Hybrid feature matrix shape: {X_train_hybrid.shape}\")\n",
    "print(f\"  - TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"  - Behavioral features: {X_train_behav.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715dd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hybrid model\n",
    "print(\"\\nTraining Hybrid Model (TF-IDF + Behavioral)...\")\n",
    "model_hybrid = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42)\n",
    "model_hybrid.fit(X_train_hybrid, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_hybrid = model_hybrid.predict(X_val_hybrid)\n",
    "y_pred_proba_hybrid = model_hybrid.predict_proba(X_val_hybrid)[:, 1]\n",
    "roc_auc_hybrid = roc_auc_score(y_val, y_pred_proba_hybrid)\n",
    "f1_hybrid = f1_score(y_val, y_pred_hybrid)\n",
    "acc_hybrid = accuracy_score(y_val, y_pred_hybrid)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HYBRID MODEL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {acc_hybrid:.4f}\")\n",
    "print(f\"F1 Score: {f1_hybrid:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_hybrid:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_hybrid, target_names=[\"Real\", \"Fake\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a1740",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Behavioral Only', 'TF-IDF (Baseline)', 'Hybrid (TF-IDF + Behavioral)'],\n",
    "    'Accuracy': [acc_behav, acc_tfidf, acc_hybrid],\n",
    "    'F1 Score': [f1_behav, f1_tfidf, f1_hybrid],\n",
    "    'ROC-AUC': [roc_auc_behav, roc_auc_tfidf, roc_auc_hybrid]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\nBest Model: {comparison.loc[comparison['ROC-AUC'].idxmax(), 'Model']} (ROC-AUC: {comparison['ROC-AUC'].max():.4f})\")\n",
    "print(f\"\\nImprovement over baseline:\")\n",
    "for idx, row in comparison.iterrows():\n",
    "    if idx > 0:\n",
    "        improvement = ((row['ROC-AUC'] - roc_auc_tfidf) / roc_auc_tfidf) * 100\n",
    "        print(f\"  {row['Model']}: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, comparison['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax.bar(x, comparison['F1 Score'], width, label='F1 Score', alpha=0.8)\n",
    "ax.bar(x + width, comparison['ROC-AUC'], width, label='ROC-AUC', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764caff",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (Behavioral Features in Hybrid Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract behavioral feature coefficients from hybrid model\n",
    "behavioral_coefs = model_hybrid.coef_[0][-len(feature_cols):]\n",
    "behavioral_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': behavioral_coefs,\n",
    "    'Abs_Coefficient': np.abs(behavioral_coefs)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nBehavioral Feature Importance (in Hybrid Model):\")\n",
    "print(behavioral_importance)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in behavioral_importance['Coefficient']]\n",
    "ax.barh(behavioral_importance['Feature'], behavioral_importance['Coefficient'], color=colors)\n",
    "ax.set_title('Behavioral Feature Importance\\n(Logistic Regression Coefficients)', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6be70",
   "metadata": {},
   "source": [
    "## 9. ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad22781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "fpr_behav, tpr_behav, _ = roc_curve(y_val, y_pred_proba_behav)\n",
    "fpr_tfidf, tpr_tfidf, _ = roc_curve(y_val, y_pred_proba_tfidf)\n",
    "fpr_hybrid, tpr_hybrid, _ = roc_curve(y_val, y_pred_proba_hybrid)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(fpr_behav, tpr_behav, color='purple', lw=2, label=f'Behavioral Only (AUC={roc_auc_behav:.3f})')\n",
    "ax.plot(fpr_tfidf, tpr_tfidf, color='darkorange', lw=2, label=f'TF-IDF (AUC={roc_auc_tfidf:.3f})')\n",
    "ax.plot(fpr_hybrid, tpr_hybrid, color='darkgreen', lw=2.5, label=f'Hybrid (AUC={roc_auc_hybrid:.3f})', linestyle='--')\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax.set_title('ROC Curve Comparison: All Models', fontweight='bold', fontsize=13)\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eae4d6",
   "metadata": {},
   "source": [
    "## 10. Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS FROM SPRINT 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement_hybrid = ((roc_auc_hybrid - roc_auc_tfidf) / roc_auc_tfidf) * 100\n",
    "improvement_behav = ((roc_auc_behav - roc_auc_tfidf) / roc_auc_tfidf) * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "PERFORMANCE:\n",
    "  • Behavioral-only achieves {roc_auc_behav:.4f} ROC-AUC ({improvement_behav:+.2f}% vs baseline)\n",
    "  • TF-IDF baseline: {roc_auc_tfidf:.4f} ROC-AUC\n",
    "  • Hybrid model: {roc_auc_hybrid:.4f} ROC-AUC ({improvement_hybrid:+.2f}% vs baseline)\n",
    "\n",
    "MOST PREDICTIVE FEATURES:\n",
    "\"\"\")\n",
    "\n",
    "top_features = behavioral_importance.head(5)\n",
    "for idx, row in top_features.iterrows():\n",
    "    direction = \"→ Real\" if row['Coefficient'] > 0 else \"→ Fake\"\n",
    "    print(f\"  • {row['Feature']}: {row['Coefficient']:.4f} {direction}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "IMPLICATIONS:\n",
    "  • Behavioral features alone are {'predictive' if roc_auc_behav > 0.65 else 'not very predictive'}\n",
    "  • Hybrid model {'improves' if improvement_hybrid > 0 else 'does not improve'} over baseline\n",
    "  • Text content (TF-IDF) remains {'the dominant signal' if roc_auc_tfidf > roc_auc_hybrid else 'secondary to linguistic style'}\n",
    "  • Next: Transform fine-tuning for semantic understanding\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
