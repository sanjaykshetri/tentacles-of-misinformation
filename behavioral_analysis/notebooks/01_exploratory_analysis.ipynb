{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a947a8",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2694f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, spearmanr, pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd17e6",
   "metadata": {},
   "source": [
    "## 2. Load & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, create synthetic data that mirrors behavioral study structure\n",
    "# In production: Load actual thesis data with df = pd.read_csv('data/raw/behavioral_survey.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "n_subjects = 194\n",
    "\n",
    "# Synthetic behavioral data mimicking cognitive assessments\n",
    "data = pd.DataFrame({\n",
    "    'subject_id': range(1, n_subjects + 1),\n",
    "    'crt_score': np.random.normal(loc=1.5, scale=0.8, size=n_subjects).clip(0, 3),\n",
    "    'nfc_score': np.random.normal(loc=3.8, scale=0.7, size=n_subjects).clip(1, 5),\n",
    "    'conspiracy_mentality': np.random.normal(loc=2.8, scale=0.9, size=n_subjects).clip(1, 5),\n",
    "    'bs_receptivity': np.random.normal(loc=2.5, scale=1.0, size=n_subjects).clip(0, 5),\n",
    "    'rational_style': np.random.normal(loc=3.5, scale=0.8, size=n_subjects).clip(1, 5),\n",
    "    'intuitive_style': np.random.normal(loc=3.2, scale=0.9, size=n_subjects).clip(1, 5),\n",
    "    'verification_behavior': np.random.normal(loc=3.0, scale=1.1, size=n_subjects).clip(0, 5),\n",
    "    'age': np.random.normal(loc=35, scale=12, size=n_subjects).clip(18, 75).astype(int),\n",
    "    'education_years': np.random.normal(loc=14, scale=2.5, size=n_subjects).clip(8, 20).astype(int),\n",
    "    'misinformation_susceptibility': np.random.normal(loc=2.5, scale=1.2, size=n_subjects).clip(0, 5)\n",
    "})\n",
    "\n",
    "# Add some missingness (realistic 5-10%)\n",
    "missing_indices = np.random.choice(len(data), size=int(0.07 * len(data)), replace=False)\n",
    "data.loc[missing_indices, 'bs_receptivity'] = np.nan\n",
    "\n",
    "print(f\"✓ Loaded synthetic behavioral data: {data.shape[0]} subjects, {data.shape[1]} features\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "print(f\"\\nData types:\\n{data.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f3726",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af38f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "missing_summary = pd.DataFrame({\n",
    "    'feature': data.columns,\n",
    "    'missing_count': data.isnull().sum(),\n",
    "    'missing_percent': (data.isnull().sum() / len(data) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_summary[missing_summary['missing_count'] > 0])\n",
    "\n",
    "# Duplicate check\n",
    "print(f\"\\nDuplicate rows: {data.duplicated().sum()}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(data.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebf321",
   "metadata": {},
   "source": [
    "## 4. Descriptive Statistics for Cognitive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9330fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on cognitive/behavioral features (excluding demographics)\n",
    "cognitive_features = [\n",
    "    'crt_score', 'nfc_score', 'conspiracy_mentality', 'bs_receptivity',\n",
    "    'rational_style', 'intuitive_style', 'verification_behavior',\n",
    "    'misinformation_susceptibility'\n",
    "]\n",
    "\n",
    "# Create comprehensive descriptive table\n",
    "descriptive_stats = pd.DataFrame({\n",
    "    'Mean': data[cognitive_features].mean().round(3),\n",
    "    'SD': data[cognitive_features].std().round(3),\n",
    "    'Median': data[cognitive_features].median().round(3),\n",
    "    'Min': data[cognitive_features].min().round(3),\n",
    "    'Max': data[cognitive_features].max().round(3),\n",
    "    'Skewness': data[cognitive_features].skew().round(3),\n",
    "    'Kurtosis': data[cognitive_features].kurtosis().round(3),\n",
    "    'N': data[cognitive_features].count()\n",
    "})\n",
    "\n",
    "print(\"Table 1: Descriptive Statistics for Cognitive Features\")\n",
    "print(\"=\"*90)\n",
    "print(descriptive_stats)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Skewness > 0 = right-skewed; < 0 = left-skewed\")\n",
    "print(\"- Kurtosis > 0 = heavier tails (leptokurtic); < 0 = lighter tails (platykurtic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f7499",
   "metadata": {},
   "source": [
    "## 5. Normality Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normality using Shapiro-Wilk (appropriate for N < 300)\n",
    "normality_results = []\n",
    "\n",
    "for feature in cognitive_features:\n",
    "    # Remove NaN for testing\n",
    "    valid_data = data[feature].dropna()\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    stat, p_value = shapiro(valid_data)\n",
    "    is_normal = 'Yes' if p_value > 0.05 else 'No'\n",
    "    \n",
    "    normality_results.append({\n",
    "        'Feature': feature,\n",
    "        'Shapiro-Wilk Stat': round(stat, 4),\n",
    "        'p-value': round(p_value, 4),\n",
    "        'Normal (α=0.05)': is_normal\n",
    "    })\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "\n",
    "print(\"Table 2: Shapiro-Wilk Normality Test\")\n",
    "print(\"=\"*80)\n",
    "print(normality_df.to_string(index=False))\n",
    "print(\"\\nNote: p-value > 0.05 suggests distribution is consistent with normality.\")\n",
    "print(f\"Non-normal features: {len(normality_df[normality_df['Normal (α=0.05)'] == 'No'])} / {len(normality_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3430b",
   "metadata": {},
   "source": [
    "## 6. Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ad548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms with normal distribution overlays\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(cognitive_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(data[feature].dropna(), bins=20, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    # Overlay normal distribution\n",
    "    mu, sigma = data[feature].mean(), data[feature].std()\n",
    "    x = np.linspace(data[feature].min(), data[feature].max(), 100)\n",
    "    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal fit')\n",
    "    \n",
    "    ax.set_title(f'{feature}\\n(μ={mu:.2f}, σ={sigma:.2f})', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/01_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: docs/figures/01_feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b23b0d",
   "metadata": {},
   "source": [
    "## 7. Q-Q Plots for Normality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plots for visual normality assessment\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(cognitive_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Remove NaN\n",
    "    valid_data = data[feature].dropna()\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(valid_data, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(f'{feature}', fontsize=10, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/02_qq_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: docs/figures/02_qq_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b665c94",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868696f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation (parametric)\n",
    "pearson_corr = data[cognitive_features].corr(method='pearson')\n",
    "\n",
    "print(\"Pearson Correlation Matrix (cognitive features):\")\n",
    "print(\"=\"*80)\n",
    "print(pearson_corr.round(3))\n",
    "\n",
    "# Spearman correlation (non-parametric, more robust)\n",
    "spearman_corr = data[cognitive_features].corr(method='spearman')\n",
    "\n",
    "print(\"\\nSpearman Correlation Matrix (cognitive features):\")\n",
    "print(\"=\"*80)\n",
    "print(spearman_corr.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93baa212",
   "metadata": {},
   "source": [
    "## 9. Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pearson_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "ax.set_title('Pearson Correlation Matrix: Cognitive Features', fontsize=12, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/03_pearson_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: docs/figures/03_pearson_correlation.png\")\n",
    "\n",
    "# Spearman heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(spearman_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "ax.set_title('Spearman Correlation Matrix: Cognitive Features', fontsize=12, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/04_spearman_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: docs/figures/04_spearman_correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255bfbaa",
   "metadata": {},
   "source": [
    "## 10. Key Correlations with Misinformation Susceptibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on correlations with target outcome\n",
    "target = 'misinformation_susceptibility'\n",
    "predictor_features = [f for f in cognitive_features if f != target]\n",
    "\n",
    "correlation_with_target = []\n",
    "\n",
    "for feature in predictor_features:\n",
    "    # Pearson\n",
    "    pearson_r, pearson_p = pearsonr(data[feature].dropna(), data[target][:len(data[feature].dropna())])\n",
    "    \n",
    "    # Spearman (more robust)\n",
    "    spearman_r, spearman_p = spearmanr(data[feature].dropna(), data[target][:len(data[feature].dropna())])\n",
    "    \n",
    "    correlation_with_target.append({\n",
    "        'Feature': feature,\n",
    "        'Pearson r': round(pearson_r, 3),\n",
    "        'Pearson p': round(pearson_p, 4),\n",
    "        'Spearman ρ': round(spearman_r, 3),\n",
    "        'Spearman p': round(spearman_p, 4)\n",
    "    })\n",
    "\n",
    "target_corr_df = pd.DataFrame(correlation_with_target).sort_values('Spearman ρ', ascending=False, key=abs)\n",
    "\n",
    "print(f\"Table 3: Correlations with {target}\")\n",
    "print(\"=\"*100)\n",
    "print(target_corr_df.to_string(index=False))\n",
    "print(\"\\nSignificant correlations (p < 0.05):\")\n",
    "print(target_corr_df[target_corr_df['Spearman p'] < 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49c5f5",
   "metadata": {},
   "source": [
    "## 11. Multivariate Structure & Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(data[cognitive_features].dropna())\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Explained variance\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(\"Explained Variance by Principal Component:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(8, len(pca.explained_variance_ratio_))):\n",
    "    print(f\"PC{i+1}: {pca.explained_variance_ratio_[i]:.1%} (Cumulative: {cumsum_var[i]:.1%})\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "ax1.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Scree Plot', fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "ax2.plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-', linewidth=2, markersize=6)\n",
    "ax2.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance')\n",
    "ax2.set_title('Cumulative Explained Variance', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/figures/05_pca_variance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Saved: docs/figures/05_pca_variance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b81b93",
   "metadata": {},
   "source": [
    "## 12. Summary & Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=\"*80\n",
    "BEHAVIORAL ANALYSIS: EXPLORATORY DATA ANALYSIS REPORT\n",
    "=\"*80\n",
    "\n",
    "SAMPLE CHARACTERISTICS:\n",
    "-\" * 80)\n",
    "print(f\"Sample size: N = {len(data)}\")\n",
    "print(f\"Age: M = {data['age'].mean():.1f}, SD = {data['age'].std():.1f}, Range = {data['age'].min()}-{data['age'].max()}\")\n",
    "print(f\"Education: M = {data['education_years'].mean():.1f} years, SD = {data['education_years'].std():.1f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "DATA QUALITY:\n",
    "-\" * 80)\n",
    "print(f\"Missing data: {data.isnull().sum().sum()} values ({data.isnull().sum().sum() / (data.shape[0] * data.shape[1]) * 100:.1f}%)\")\n",
    "print(f\"Duplicates: 0\")\n",
    "\n",
    "print(f\"\"\"\n",
    "NORMALITY:\n",
    "-\" * 80)\n",
    "normal_count = len(normality_df[normality_df['Normal (α=0.05)'] == 'Yes'])\n",
    "print(f\"Features with normal distribution: {normal_count}/{len(normality_df)}\")\n",
    "print(f\"Non-normal features: {normality_df[normality_df['Normal (α=0.05)'] == 'No']['Feature'].tolist()}\")\n",
    "print(\"\\nRecommendation: Use both parametric (Pearson) and non-parametric (Spearman) methods.\")\n",
    "\n",
    "print(f\"\"\"\n",
    "KEY CORRELATIONS WITH MISINFORMATION SUSCEPTIBILITY:\n",
    "-\" * 80)\n",
    "sig_correlations = target_corr_df[target_corr_df['Spearman p'] < 0.05]\n",
    "if len(sig_correlations) > 0:\n",
    "    for idx, row in sig_correlations.iterrows():\n",
    "        print(f\"  - {row['Feature']}: ρ = {row['Spearman ρ']:.3f}, p = {row['Spearman p']:.4f}\")\n",
    "else:\n",
    "    print(\"  No statistically significant correlations at α = 0.05\")\n",
    "\n",
    "print(f\"\"\"\n",
    "DIMENSIONALITY:\n",
    "-\" * 80)\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "print(f\"Components needed for 95% variance explained: {n_components_95}\")\n",
    "print(f\"Total features: {len(cognitive_features)}\")\n",
    "print(f\"Dimensionality reduction potential: Yes, consider PCA or feature selection\")\n",
    "\n",
    "print(f\"\"\"\n",
    "NEXT STEPS:\n",
    "-\" * 80)\n",
    "print(\"1. Handle missing data (imputation or deletion)\")\n",
    "print(\"2. Create feature composites if warranted by theory\")\n",
    "print(\"3. Standardize features for regression modeling\")\n",
    "print(\"4. Proceed to feature engineering (02_feature_engineering.ipynb)\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5875aaf",
   "metadata": {},
   "source": [
    "## 13. Export Clean Dataset for Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "data_clean = data.dropna()\n",
    "data_clean.to_csv('data/processed/behavioral_features_clean.csv', index=False)\n",
    "\n",
    "print(f\"✓ Saved clean dataset: {len(data_clean)} subjects, {len(data_clean.columns)} features\")\n",
    "print(f\"  Path: data/processed/behavioral_features_clean.csv\")\n",
    "\n",
    "# Save descriptive statistics for manuscript\n",
    "descriptive_stats.to_csv('behavioral_analysis/results/01_descriptive_statistics.csv')\n",
    "print(f\"\\n✓ Saved descriptive statistics: behavioral_analysis/results/01_descriptive_statistics.csv\")\n",
    "\n",
    "# Save normality results\n",
    "normality_df.to_csv('behavioral_analysis/results/02_normality_tests.csv', index=False)\n",
    "print(f\"✓ Saved normality results: behavioral_analysis/results/02_normality_tests.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
